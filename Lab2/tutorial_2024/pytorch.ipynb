{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What is PyTorch?\n",
    "================\n",
    "https://pytorch.org/docs/stable/index.html\n",
    "\n",
    "It’s a Python-based scientific computing package targeted at two sets of\n",
    "audiences:\n",
    "\n",
    "-  A replacement for NumPy to use the power of GPUs\n",
    "-  a deep learning research platform that provides maximum flexibility\n",
    "   and speed\n",
    "\n",
    "\n",
    "NumPy Bridge\n",
    "------------\n",
    "1. Tensors are similar to NumPy’s arrays, with the addition being that\n",
    "Tensors can also be used on a GPU to accelerate computing.  \n",
    "\n",
    "2. All the Tensors on the CPU except a CharTensor support converting to\n",
    "NumPy and back. Converting a Torch Tensor to a NumPy array and vice versa is a breeze.  \n",
    "\n",
    "3. The Torch Tensor and NumPy array will share their underlying memory\n",
    "locations (if the Torch Tensor is on CPU), and changing one will change\n",
    "the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine your GPUs are available for pytorch (False for CPU version of pytorch)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### array <-> tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "# tensor to array\n",
    "x_tensor = torch.tensor(data); print(type(x_tensor))\n",
    "x_tensor2array = x_tensor.numpy(); print(type(x_tensor2array)); print()\n",
    "# array to tensor\n",
    "x_array = np.array(data); print(type(x_array))\n",
    "x_array2tenor = torch.from_numpy(x_array); print(type(x_array2tenor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x = torch.tensor(data); print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[ 1.9045, -1.5745, -0.1813],\n",
      "        [ 0.5976,  3.0659, -0.0707]])\n",
      "tensor([[0.8375, 0.5611, 0.6663],\n",
      "        [0.8667, 0.5797, 0.0455]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0.1645, 0.2558, 0.0829],\n",
      "        [0.7729, 0.0047, 0.4612]])\n"
     ]
    }
   ],
   "source": [
    "# empty/rand/ones/zeros/eye(), xxx_like()\n",
    "# the difference is you can fill dimensions without brackets at the beginning for these functions\n",
    "x = torch.empty(2, 3); print(x)\n",
    "x = torch.randn(2, 3); print(x) # initialization from the Gaussian distribution of N(0,1)\n",
    "x = torch.rand(2, 3); print(x)  # initialization from (0,1)\n",
    "# cuda() means translating your tensors into GPUs\n",
    "x = torch.zeros(2, 3, dtype=torch.float); print(x) # if your device is CPU, please delete the cuda()\n",
    "y = torch.rand_like(x) # _like means the same shape, data type and device(GPU or CPU)\n",
    "print(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# get shape\n",
    "print(x.shape)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a one element tensor, use ``.item()`` to get the value as a Python number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1309])\n",
      "-1.130883812904358\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A bunch of operations in numpy have corresponding functions in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10)\n",
    "# torch.stack()\n",
    "# torch.concatenate()\n",
    "# torch.squeeze()/unsqueeze()\n",
    "# torch.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# permute -> transpose in numpy, to switch the order of dimensions\n",
    "x = torch.arange(24).reshape(2,3,4)\n",
    "y = x.permute([2,1,0]); print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of Tensors\n",
    "Note: The default calculation is element-wise calculation (+ - * /)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[0.4255, 1.3901, 2.4775],\n",
      "        [3.0277, 4.0986, 5.7243]])\n",
      "tensor([[0.0000, 0.3901, 0.9551],\n",
      "        [0.0832, 0.3944, 3.6214]])\n",
      "tensor([[  0.0000,   2.5633,   4.1881],\n",
      "        [108.2108,  40.5668,   6.9034]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6).reshape(2, 3); print(x)\n",
    "y = torch.rand(2, 3); print(x + y)\n",
    "print(x * y)\n",
    "print(x / y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch.mul() equals \"*\": Element-wise multiplication\n",
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "print(torch.mul(a, b).size())\n",
    "print((a * b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch.mm(): the common mathematical matrix multiplication\n",
    "mat1 = torch.randn(2, 3)\n",
    "mat2 = torch.randn(3, 3)\n",
    "print(torch.mm(mat1, mat2).size())\n",
    "print((mat1 @ mat2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.matmul() equals \"@\": Matrix product of two tensors, it includes matrix multiplication methods of different dimensions\n",
    "tensor1 = torch.randn(10, 3, 4)\n",
    "tensor2 = torch.randn(10, 4, 5)\n",
    "torch.matmul(tensor1, tensor2).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read later:**\n",
    "\n",
    "\n",
    "  100+ Tensor operations, including transposing, indexing, slicing,\n",
    "  mathematical operations, linear algebra, random numbers, etc.,\n",
    "  are described [here](https://pytorch.org/docs/torch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA Tensors\n",
    "------------\n",
    "\n",
    "Tensors can be moved onto any device using the ``.to`` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "x = torch.arange(6).reshape(2,3) # x created in CPU\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "# y.cuda() is also an easy method to move onto the default GPU\n",
    "x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "z = x + y\n",
    "print(z)\n",
    "print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise1\n",
    "Broadcasting is an underlying rules for dealing with unpaired tensors.  \n",
    "[Broadcasting tutorial](https://deeplearninguniversity.com/pytorch/pytorch-broadcasting/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([2, 1])\n",
      "torch.Size([1])\n",
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.tensor([[1, 2], [0, 3]])\n",
    "tensor2 = torch.tensor([[3, 1]])\n",
    "tensor3 = torch.tensor([[5], [2]])\n",
    "tensor4 = torch.tensor([7])\n",
    "\n",
    "print(tensor1.shape)  # Outputs- torch.Size([2, 2])\n",
    "print(tensor2.shape)  # Outputs- torch.Size([1, 2])\n",
    "print(tensor3.shape)  # Outputs- torch.Size([2, 1])\n",
    "print(tensor4.shape)  # Outputs- torch.Size([1])\n",
    "\n",
    "print(tensor1 + tensor2)  # Outputs- tensor([[4, 3], [3, 4]])\n",
    "print(tensor1 + tensor3)  # Outputs- tensor([[6, 7], [2, 5]])\n",
    "print(tensor2 + tensor3)  # Outputs- tensor([[8, 6], [5, 3]])\n",
    "print(tensor1 + tensor4)  # Outputs- tensor([[ 8, 9], [ 7, 10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise2\n",
    "[Pytorch official website](https://pytorch.org/tutorials/) has provided sufficient tutorials for beginners.\n",
    "Familiarize yourself with PyTorch concepts and modules. Learn how to load data, build deep neural networks, train and save your models in [quickstart guide](https://pytorch.org/tutorials/beginner/basics/intro.html).\n",
    "\n",
    "We also provide old version of the pytorch tutorials in file folder \"old_pytorch_tutorials\". The tutorials in [Pytorch official website](https://pytorch.org/tutorials/) is more recommended.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.299962  [   64/60000]\n",
      "loss: 2.292246  [ 6464/60000]\n",
      "loss: 2.276347  [12864/60000]\n",
      "loss: 2.270225  [19264/60000]\n",
      "loss: 2.251564  [25664/60000]\n",
      "loss: 2.212792  [32064/60000]\n",
      "loss: 2.229574  [38464/60000]\n",
      "loss: 2.186539  [44864/60000]\n",
      "loss: 2.193128  [51264/60000]\n",
      "loss: 2.164626  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.8%, Avg loss: 2.159079 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.165825  [   64/60000]\n",
      "loss: 2.157677  [ 6464/60000]\n",
      "loss: 2.103598  [12864/60000]\n",
      "loss: 2.124600  [19264/60000]\n",
      "loss: 2.068831  [25664/60000]\n",
      "loss: 2.005532  [32064/60000]\n",
      "loss: 2.034812  [38464/60000]\n",
      "loss: 1.948631  [44864/60000]\n",
      "loss: 1.961962  [51264/60000]\n",
      "loss: 1.894032  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 1.893828 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.916904  [   64/60000]\n",
      "loss: 1.890345  [ 6464/60000]\n",
      "loss: 1.777756  [12864/60000]\n",
      "loss: 1.826362  [19264/60000]\n",
      "loss: 1.714229  [25664/60000]\n",
      "loss: 1.662651  [32064/60000]\n",
      "loss: 1.684667  [38464/60000]\n",
      "loss: 1.583755  [44864/60000]\n",
      "loss: 1.617016  [51264/60000]\n",
      "loss: 1.508515  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Avg loss: 1.532110 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.589831  [   64/60000]\n",
      "loss: 1.558869  [ 6464/60000]\n",
      "loss: 1.413799  [12864/60000]\n",
      "loss: 1.489746  [19264/60000]\n",
      "loss: 1.372467  [25664/60000]\n",
      "loss: 1.366128  [32064/60000]\n",
      "loss: 1.378062  [38464/60000]\n",
      "loss: 1.302595  [44864/60000]\n",
      "loss: 1.345428  [51264/60000]\n",
      "loss: 1.234172  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Avg loss: 1.267296 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.338349  [   64/60000]\n",
      "loss: 1.322115  [ 6464/60000]\n",
      "loss: 1.159931  [12864/60000]\n",
      "loss: 1.266939  [19264/60000]\n",
      "loss: 1.142785  [25664/60000]\n",
      "loss: 1.169308  [32064/60000]\n",
      "loss: 1.185429  [38464/60000]\n",
      "loss: 1.123719  [44864/60000]\n",
      "loss: 1.170397  [51264/60000]\n",
      "loss: 1.069226  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 1.099416 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.164924  [   64/60000]\n",
      "loss: 1.168327  [ 6464/60000]\n",
      "loss: 0.990004  [12864/60000]\n",
      "loss: 1.125693  [19264/60000]\n",
      "loss: 1.001307  [25664/60000]\n",
      "loss: 1.035593  [32064/60000]\n",
      "loss: 1.064545  [38464/60000]\n",
      "loss: 1.007935  [44864/60000]\n",
      "loss: 1.055053  [51264/60000]\n",
      "loss: 0.964660  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 0.990384 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.042397  [   64/60000]\n",
      "loss: 1.067396  [ 6464/60000]\n",
      "loss: 0.873207  [12864/60000]\n",
      "loss: 1.031898  [19264/60000]\n",
      "loss: 0.913449  [25664/60000]\n",
      "loss: 0.941021  [32064/60000]\n",
      "loss: 0.985710  [38464/60000]\n",
      "loss: 0.932188  [44864/60000]\n",
      "loss: 0.974652  [51264/60000]\n",
      "loss: 0.895249  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 0.916063 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.951980  [   64/60000]\n",
      "loss: 0.997397  [ 6464/60000]\n",
      "loss: 0.789391  [12864/60000]\n",
      "loss: 0.965662  [19264/60000]\n",
      "loss: 0.855379  [25664/60000]\n",
      "loss: 0.871581  [32064/60000]\n",
      "loss: 0.930371  [38464/60000]\n",
      "loss: 0.881037  [44864/60000]\n",
      "loss: 0.915662  [51264/60000]\n",
      "loss: 0.845537  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.862345 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.882464  [   64/60000]\n",
      "loss: 0.945372  [ 6464/60000]\n",
      "loss: 0.726743  [12864/60000]\n",
      "loss: 0.916840  [19264/60000]\n",
      "loss: 0.814075  [25664/60000]\n",
      "loss: 0.819389  [32064/60000]\n",
      "loss: 0.888584  [38464/60000]\n",
      "loss: 0.845247  [44864/60000]\n",
      "loss: 0.871272  [51264/60000]\n",
      "loss: 0.808071  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.821924 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.827400  [   64/60000]\n",
      "loss: 0.904372  [ 6464/60000]\n",
      "loss: 0.678255  [12864/60000]\n",
      "loss: 0.879380  [19264/60000]\n",
      "loss: 0.783305  [25664/60000]\n",
      "loss: 0.779527  [32064/60000]\n",
      "loss: 0.855201  [38464/60000]\n",
      "loss: 0.818916  [44864/60000]\n",
      "loss: 0.836695  [51264/60000]\n",
      "loss: 0.778227  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.790094 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.782487  [   64/60000]\n",
      "loss: 0.870141  [ 6464/60000]\n",
      "loss: 0.639502  [12864/60000]\n",
      "loss: 0.849786  [19264/60000]\n",
      "loss: 0.758979  [25664/60000]\n",
      "loss: 0.748270  [32064/60000]\n",
      "loss: 0.826917  [38464/60000]\n",
      "loss: 0.798482  [44864/60000]\n",
      "loss: 0.808754  [51264/60000]\n",
      "loss: 0.753314  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.763872 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.744718  [   64/60000]\n",
      "loss: 0.840437  [ 6464/60000]\n",
      "loss: 0.607410  [12864/60000]\n",
      "loss: 0.825657  [19264/60000]\n",
      "loss: 0.738630  [25664/60000]\n",
      "loss: 0.723001  [32064/60000]\n",
      "loss: 0.801775  [38464/60000]\n",
      "loss: 0.781484  [44864/60000]\n",
      "loss: 0.785475  [51264/60000]\n",
      "loss: 0.731885  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.0%, Avg loss: 0.741474 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.712128  [   64/60000]\n",
      "loss: 0.813797  [ 6464/60000]\n",
      "loss: 0.580165  [12864/60000]\n",
      "loss: 0.805305  [19264/60000]\n",
      "loss: 0.720822  [25664/60000]\n",
      "loss: 0.702173  [32064/60000]\n",
      "loss: 0.779011  [38464/60000]\n",
      "loss: 0.766694  [44864/60000]\n",
      "loss: 0.765614  [51264/60000]\n",
      "loss: 0.712856  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.721745 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.683402  [   64/60000]\n",
      "loss: 0.789482  [ 6464/60000]\n",
      "loss: 0.556600  [12864/60000]\n",
      "loss: 0.787570  [19264/60000]\n",
      "loss: 0.705016  [25664/60000]\n",
      "loss: 0.684591  [32064/60000]\n",
      "loss: 0.758021  [38464/60000]\n",
      "loss: 0.753494  [44864/60000]\n",
      "loss: 0.748157  [51264/60000]\n",
      "loss: 0.695646  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.703989 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.657946  [   64/60000]\n",
      "loss: 0.767090  [ 6464/60000]\n",
      "loss: 0.535775  [12864/60000]\n",
      "loss: 0.771734  [19264/60000]\n",
      "loss: 0.690766  [25664/60000]\n",
      "loss: 0.669509  [32064/60000]\n",
      "loss: 0.738425  [38464/60000]\n",
      "loss: 0.741412  [44864/60000]\n",
      "loss: 0.732666  [51264/60000]\n",
      "loss: 0.679854  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.687828 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.635031  [   64/60000]\n",
      "loss: 0.746464  [ 6464/60000]\n",
      "loss: 0.517277  [12864/60000]\n",
      "loss: 0.757406  [19264/60000]\n",
      "loss: 0.677960  [25664/60000]\n",
      "loss: 0.656360  [32064/60000]\n",
      "loss: 0.720026  [38464/60000]\n",
      "loss: 0.730293  [44864/60000]\n",
      "loss: 0.718854  [51264/60000]\n",
      "loss: 0.665323  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.672996 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.614271  [   64/60000]\n",
      "loss: 0.727452  [ 6464/60000]\n",
      "loss: 0.500798  [12864/60000]\n",
      "loss: 0.744199  [19264/60000]\n",
      "loss: 0.666404  [25664/60000]\n",
      "loss: 0.644715  [32064/60000]\n",
      "loss: 0.702825  [38464/60000]\n",
      "loss: 0.720173  [44864/60000]\n",
      "loss: 0.706504  [51264/60000]\n",
      "loss: 0.651800  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 0.659296 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.595369  [   64/60000]\n",
      "loss: 0.709965  [ 6464/60000]\n",
      "loss: 0.485796  [12864/60000]\n",
      "loss: 0.732042  [19264/60000]\n",
      "loss: 0.655870  [25664/60000]\n",
      "loss: 0.634287  [32064/60000]\n",
      "loss: 0.686899  [38464/60000]\n",
      "loss: 0.711176  [44864/60000]\n",
      "loss: 0.695378  [51264/60000]\n",
      "loss: 0.639265  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.5%, Avg loss: 0.646619 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.578133  [   64/60000]\n",
      "loss: 0.693806  [ 6464/60000]\n",
      "loss: 0.472188  [12864/60000]\n",
      "loss: 0.720657  [19264/60000]\n",
      "loss: 0.646464  [25664/60000]\n",
      "loss: 0.624987  [32064/60000]\n",
      "loss: 0.671999  [38464/60000]\n",
      "loss: 0.703211  [44864/60000]\n",
      "loss: 0.685686  [51264/60000]\n",
      "loss: 0.627752  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.634947 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.562379  [   64/60000]\n",
      "loss: 0.678675  [ 6464/60000]\n",
      "loss: 0.459849  [12864/60000]\n",
      "loss: 0.710058  [19264/60000]\n",
      "loss: 0.637811  [25664/60000]\n",
      "loss: 0.616615  [32064/60000]\n",
      "loss: 0.658110  [38464/60000]\n",
      "loss: 0.696267  [44864/60000]\n",
      "loss: 0.677293  [51264/60000]\n",
      "loss: 0.616909  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.3%, Avg loss: 0.624189 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.547918  [   64/60000]\n",
      "loss: 0.664742  [ 6464/60000]\n",
      "loss: 0.448550  [12864/60000]\n",
      "loss: 0.700113  [19264/60000]\n",
      "loss: 0.629764  [25664/60000]\n",
      "loss: 0.608880  [32064/60000]\n",
      "loss: 0.645231  [38464/60000]\n",
      "loss: 0.690217  [44864/60000]\n",
      "loss: 0.669997  [51264/60000]\n",
      "loss: 0.606728  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.614269 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.534585  [   64/60000]\n",
      "loss: 0.651871  [ 6464/60000]\n",
      "loss: 0.438171  [12864/60000]\n",
      "loss: 0.690770  [19264/60000]\n",
      "loss: 0.622173  [25664/60000]\n",
      "loss: 0.601729  [32064/60000]\n",
      "loss: 0.633280  [38464/60000]\n",
      "loss: 0.685140  [44864/60000]\n",
      "loss: 0.663629  [51264/60000]\n",
      "loss: 0.597047  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.605105 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.522203  [   64/60000]\n",
      "loss: 0.639971  [ 6464/60000]\n",
      "loss: 0.428618  [12864/60000]\n",
      "loss: 0.681987  [19264/60000]\n",
      "loss: 0.614978  [25664/60000]\n",
      "loss: 0.595092  [32064/60000]\n",
      "loss: 0.622231  [38464/60000]\n",
      "loss: 0.680855  [44864/60000]\n",
      "loss: 0.658081  [51264/60000]\n",
      "loss: 0.587816  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.596627 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.510712  [   64/60000]\n",
      "loss: 0.628962  [ 6464/60000]\n",
      "loss: 0.419786  [12864/60000]\n",
      "loss: 0.673682  [19264/60000]\n",
      "loss: 0.607996  [25664/60000]\n",
      "loss: 0.588839  [32064/60000]\n",
      "loss: 0.611967  [38464/60000]\n",
      "loss: 0.677382  [44864/60000]\n",
      "loss: 0.653227  [51264/60000]\n",
      "loss: 0.578903  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.588773 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.500038  [   64/60000]\n",
      "loss: 0.618785  [ 6464/60000]\n",
      "loss: 0.411574  [12864/60000]\n",
      "loss: 0.665740  [19264/60000]\n",
      "loss: 0.601295  [25664/60000]\n",
      "loss: 0.582872  [32064/60000]\n",
      "loss: 0.602456  [38464/60000]\n",
      "loss: 0.674580  [44864/60000]\n",
      "loss: 0.648998  [51264/60000]\n",
      "loss: 0.570320  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.581489 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.490051  [   64/60000]\n",
      "loss: 0.609327  [ 6464/60000]\n",
      "loss: 0.403939  [12864/60000]\n",
      "loss: 0.658189  [19264/60000]\n",
      "loss: 0.594766  [25664/60000]\n",
      "loss: 0.577137  [32064/60000]\n",
      "loss: 0.593668  [38464/60000]\n",
      "loss: 0.672360  [44864/60000]\n",
      "loss: 0.645387  [51264/60000]\n",
      "loss: 0.562102  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.574725 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.480712  [   64/60000]\n",
      "loss: 0.600585  [ 6464/60000]\n",
      "loss: 0.396790  [12864/60000]\n",
      "loss: 0.650991  [19264/60000]\n",
      "loss: 0.588337  [25664/60000]\n",
      "loss: 0.571637  [32064/60000]\n",
      "loss: 0.585544  [38464/60000]\n",
      "loss: 0.670711  [44864/60000]\n",
      "loss: 0.642245  [51264/60000]\n",
      "loss: 0.554190  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 0.568428 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.471927  [   64/60000]\n",
      "loss: 0.592440  [ 6464/60000]\n",
      "loss: 0.390080  [12864/60000]\n",
      "loss: 0.644136  [19264/60000]\n",
      "loss: 0.581993  [25664/60000]\n",
      "loss: 0.566292  [32064/60000]\n",
      "loss: 0.577995  [38464/60000]\n",
      "loss: 0.669514  [44864/60000]\n",
      "loss: 0.639443  [51264/60000]\n",
      "loss: 0.546549  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.4%, Avg loss: 0.562554 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.463713  [   64/60000]\n",
      "loss: 0.584822  [ 6464/60000]\n",
      "loss: 0.383782  [12864/60000]\n",
      "loss: 0.637581  [19264/60000]\n",
      "loss: 0.575687  [25664/60000]\n",
      "loss: 0.561081  [32064/60000]\n",
      "loss: 0.571000  [38464/60000]\n",
      "loss: 0.668708  [44864/60000]\n",
      "loss: 0.636970  [51264/60000]\n",
      "loss: 0.539118  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.557069 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.455995  [   64/60000]\n",
      "loss: 0.577760  [ 6464/60000]\n",
      "loss: 0.377886  [12864/60000]\n",
      "loss: 0.631260  [19264/60000]\n",
      "loss: 0.569451  [25664/60000]\n",
      "loss: 0.555994  [32064/60000]\n",
      "loss: 0.564499  [38464/60000]\n",
      "loss: 0.668168  [44864/60000]\n",
      "loss: 0.634687  [51264/60000]\n",
      "loss: 0.531901  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.551934 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.448721  [   64/60000]\n",
      "loss: 0.571226  [ 6464/60000]\n",
      "loss: 0.372382  [12864/60000]\n",
      "loss: 0.625142  [19264/60000]\n",
      "loss: 0.563277  [25664/60000]\n",
      "loss: 0.551010  [32064/60000]\n",
      "loss: 0.558417  [38464/60000]\n",
      "loss: 0.667833  [44864/60000]\n",
      "loss: 0.632584  [51264/60000]\n",
      "loss: 0.524841  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.547117 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.441845  [   64/60000]\n",
      "loss: 0.565128  [ 6464/60000]\n",
      "loss: 0.367194  [12864/60000]\n",
      "loss: 0.619231  [19264/60000]\n",
      "loss: 0.557177  [25664/60000]\n",
      "loss: 0.546119  [32064/60000]\n",
      "loss: 0.552720  [38464/60000]\n",
      "loss: 0.667666  [44864/60000]\n",
      "loss: 0.630562  [51264/60000]\n",
      "loss: 0.518029  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.542595 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.435330  [   64/60000]\n",
      "loss: 0.559413  [ 6464/60000]\n",
      "loss: 0.362310  [12864/60000]\n",
      "loss: 0.613514  [19264/60000]\n",
      "loss: 0.551195  [25664/60000]\n",
      "loss: 0.541346  [32064/60000]\n",
      "loss: 0.547349  [38464/60000]\n",
      "loss: 0.667607  [44864/60000]\n",
      "loss: 0.628616  [51264/60000]\n",
      "loss: 0.511428  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.538337 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.429129  [   64/60000]\n",
      "loss: 0.554082  [ 6464/60000]\n",
      "loss: 0.357709  [12864/60000]\n",
      "loss: 0.607984  [19264/60000]\n",
      "loss: 0.545326  [25664/60000]\n",
      "loss: 0.536689  [32064/60000]\n",
      "loss: 0.542315  [38464/60000]\n",
      "loss: 0.667567  [44864/60000]\n",
      "loss: 0.626738  [51264/60000]\n",
      "loss: 0.505073  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.4%, Avg loss: 0.534326 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.423238  [   64/60000]\n",
      "loss: 0.549148  [ 6464/60000]\n",
      "loss: 0.353354  [12864/60000]\n",
      "loss: 0.602625  [19264/60000]\n",
      "loss: 0.539540  [25664/60000]\n",
      "loss: 0.532145  [32064/60000]\n",
      "loss: 0.537568  [38464/60000]\n",
      "loss: 0.667536  [44864/60000]\n",
      "loss: 0.624902  [51264/60000]\n",
      "loss: 0.498932  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.530537 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.417619  [   64/60000]\n",
      "loss: 0.544527  [ 6464/60000]\n",
      "loss: 0.349241  [12864/60000]\n",
      "loss: 0.597428  [19264/60000]\n",
      "loss: 0.533930  [25664/60000]\n",
      "loss: 0.527723  [32064/60000]\n",
      "loss: 0.533022  [38464/60000]\n",
      "loss: 0.667510  [44864/60000]\n",
      "loss: 0.623083  [51264/60000]\n",
      "loss: 0.493010  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.526953 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.412214  [   64/60000]\n",
      "loss: 0.540165  [ 6464/60000]\n",
      "loss: 0.345332  [12864/60000]\n",
      "loss: 0.592475  [19264/60000]\n",
      "loss: 0.528413  [25664/60000]\n",
      "loss: 0.523398  [32064/60000]\n",
      "loss: 0.528718  [38464/60000]\n",
      "loss: 0.667402  [44864/60000]\n",
      "loss: 0.621279  [51264/60000]\n",
      "loss: 0.487363  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.7%, Avg loss: 0.523549 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.407063  [   64/60000]\n",
      "loss: 0.536030  [ 6464/60000]\n",
      "loss: 0.341625  [12864/60000]\n",
      "loss: 0.587772  [19264/60000]\n",
      "loss: 0.523028  [25664/60000]\n",
      "loss: 0.519186  [32064/60000]\n",
      "loss: 0.524634  [38464/60000]\n",
      "loss: 0.667225  [44864/60000]\n",
      "loss: 0.619449  [51264/60000]\n",
      "loss: 0.481939  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.520315 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.402090  [   64/60000]\n",
      "loss: 0.532220  [ 6464/60000]\n",
      "loss: 0.338085  [12864/60000]\n",
      "loss: 0.583248  [19264/60000]\n",
      "loss: 0.517740  [25664/60000]\n",
      "loss: 0.515089  [32064/60000]\n",
      "loss: 0.520761  [38464/60000]\n",
      "loss: 0.666972  [44864/60000]\n",
      "loss: 0.617718  [51264/60000]\n",
      "loss: 0.476876  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.517246 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.397280  [   64/60000]\n",
      "loss: 0.528683  [ 6464/60000]\n",
      "loss: 0.334792  [12864/60000]\n",
      "loss: 0.578901  [19264/60000]\n",
      "loss: 0.512611  [25664/60000]\n",
      "loss: 0.511094  [32064/60000]\n",
      "loss: 0.517083  [38464/60000]\n",
      "loss: 0.666654  [44864/60000]\n",
      "loss: 0.615914  [51264/60000]\n",
      "loss: 0.472090  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.514321 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.392622  [   64/60000]\n",
      "loss: 0.525358  [ 6464/60000]\n",
      "loss: 0.331725  [12864/60000]\n",
      "loss: 0.574704  [19264/60000]\n",
      "loss: 0.507728  [25664/60000]\n",
      "loss: 0.507212  [32064/60000]\n",
      "loss: 0.513611  [38464/60000]\n",
      "loss: 0.666177  [44864/60000]\n",
      "loss: 0.614136  [51264/60000]\n",
      "loss: 0.467539  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.511535 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.388111  [   64/60000]\n",
      "loss: 0.522249  [ 6464/60000]\n",
      "loss: 0.328698  [12864/60000]\n",
      "loss: 0.570607  [19264/60000]\n",
      "loss: 0.502941  [25664/60000]\n",
      "loss: 0.503419  [32064/60000]\n",
      "loss: 0.510297  [38464/60000]\n",
      "loss: 0.665532  [44864/60000]\n",
      "loss: 0.612351  [51264/60000]\n",
      "loss: 0.463193  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.9%, Avg loss: 0.508883 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.383710  [   64/60000]\n",
      "loss: 0.519317  [ 6464/60000]\n",
      "loss: 0.325749  [12864/60000]\n",
      "loss: 0.566659  [19264/60000]\n",
      "loss: 0.498291  [25664/60000]\n",
      "loss: 0.499715  [32064/60000]\n",
      "loss: 0.507126  [38464/60000]\n",
      "loss: 0.664792  [44864/60000]\n",
      "loss: 0.610494  [51264/60000]\n",
      "loss: 0.459056  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.506348 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.379462  [   64/60000]\n",
      "loss: 0.516536  [ 6464/60000]\n",
      "loss: 0.322909  [12864/60000]\n",
      "loss: 0.562830  [19264/60000]\n",
      "loss: 0.493768  [25664/60000]\n",
      "loss: 0.496114  [32064/60000]\n",
      "loss: 0.504091  [38464/60000]\n",
      "loss: 0.663929  [44864/60000]\n",
      "loss: 0.608624  [51264/60000]\n",
      "loss: 0.455188  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.503922 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.375311  [   64/60000]\n",
      "loss: 0.513842  [ 6464/60000]\n",
      "loss: 0.320181  [12864/60000]\n",
      "loss: 0.559053  [19264/60000]\n",
      "loss: 0.489336  [25664/60000]\n",
      "loss: 0.492695  [32064/60000]\n",
      "loss: 0.501191  [38464/60000]\n",
      "loss: 0.662931  [44864/60000]\n",
      "loss: 0.606837  [51264/60000]\n",
      "loss: 0.451498  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.501593 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.371279  [   64/60000]\n",
      "loss: 0.511260  [ 6464/60000]\n",
      "loss: 0.317539  [12864/60000]\n",
      "loss: 0.555388  [19264/60000]\n",
      "loss: 0.485092  [25664/60000]\n",
      "loss: 0.489461  [32064/60000]\n",
      "loss: 0.498407  [38464/60000]\n",
      "loss: 0.661821  [44864/60000]\n",
      "loss: 0.604949  [51264/60000]\n",
      "loss: 0.448009  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.499351 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.367376  [   64/60000]\n",
      "loss: 0.508784  [ 6464/60000]\n",
      "loss: 0.315020  [12864/60000]\n",
      "loss: 0.551929  [19264/60000]\n",
      "loss: 0.481024  [25664/60000]\n",
      "loss: 0.486371  [32064/60000]\n",
      "loss: 0.495627  [38464/60000]\n",
      "loss: 0.660679  [44864/60000]\n",
      "loss: 0.603086  [51264/60000]\n",
      "loss: 0.444680  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.497201 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.363602  [   64/60000]\n",
      "loss: 0.506411  [ 6464/60000]\n",
      "loss: 0.312593  [12864/60000]\n",
      "loss: 0.548589  [19264/60000]\n",
      "loss: 0.477091  [25664/60000]\n",
      "loss: 0.483389  [32064/60000]\n",
      "loss: 0.493004  [38464/60000]\n",
      "loss: 0.659398  [44864/60000]\n",
      "loss: 0.601193  [51264/60000]\n",
      "loss: 0.441555  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.495140 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.359950  [   64/60000]\n",
      "loss: 0.504162  [ 6464/60000]\n",
      "loss: 0.310229  [12864/60000]\n",
      "loss: 0.545414  [19264/60000]\n",
      "loss: 0.473223  [25664/60000]\n",
      "loss: 0.480496  [32064/60000]\n",
      "loss: 0.490501  [38464/60000]\n",
      "loss: 0.658020  [44864/60000]\n",
      "loss: 0.599314  [51264/60000]\n",
      "loss: 0.438584  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.493158 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.356405  [   64/60000]\n",
      "loss: 0.501999  [ 6464/60000]\n",
      "loss: 0.307955  [12864/60000]\n",
      "loss: 0.542371  [19264/60000]\n",
      "loss: 0.469510  [25664/60000]\n",
      "loss: 0.477720  [32064/60000]\n",
      "loss: 0.488031  [38464/60000]\n",
      "loss: 0.656587  [44864/60000]\n",
      "loss: 0.597528  [51264/60000]\n",
      "loss: 0.435740  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.491246 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
